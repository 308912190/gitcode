# 11.1复习model base  
## 值迭代算法（value iteration algorithm）：第三章的贝尔曼最优公式中，求解贝尔曼最优公式的算法实际上就是值迭代算法。  
1.列q 表（q-table），q(s, a) 的表达式  
2.k=0，先选取 v0=0，可以任意选取，然后把 v0 带入刚才的 q-table 当中去；
3.进行策略更新，针对每一个状态，去看哪个 qk 是最大的，那么它对应的新的策略就可以求出。  
4.k=1,进行价值更新，上面选出的最大的 qk，作为新的 v1 进行下一步的使用.
5.循环
## 策略迭代算法  
1.列q 表（q-table），q(s, a) 的表达式
2.给定随机初始策略 $$π_0$$，得到 $$π_0$$ 对应的 state value v_π_0，这样的过程就叫策略评估  
3.进行策略更新，针对每一个状态，去看哪个 qk 是最大的
4....后续循环

# 蒙特卡洛方法  
从 model-based 强化学习过渡到 model-free 的强化学习，最难以理解的就是在没有模型的情况下去估计一些量。  
最简单的方法：蒙特卡洛估算（Monte Carlo estimation），期望用x平均值来近似。 当 N 较小时，近似值不准确。随着 N 的增大，近似值会越来越精确。  
## MC Basic algorithm  
1.策略评估： 对对任意的 s 和任意的 a，计算 q_πk (s, a)  
例.复杂情况下，从一个（s1,a1）出发的话，要找 N 个轨迹，对这 N 条轨迹的 return 求平均，这样才能求出 $$q_πk(s1,a1)$$  
2.策略改进：比较一下 $$q_πk$$ 对应的 action value 哪个最大，选用对应action  
注：  
当episode 的长度较短时，只有靠近目标的状态具有非零状态值，只有离目标比较近的状态才能在这么短的步骤内找到目标，因此这些状态能找到最优策略。  
随着 episode 的长度逐渐增加，离目标越来越远的状态也能慢慢到达目标，从而找到最优策略。距离目标较近的状态比距离目标较远的状态更早出现非零状态值。  
episode 的长度应足够长，让所有状态都能有机会到达目标。  
## 改进
前提：从理论上讲，只有对每个状态（state）的每个动作值（action value）都进行了充分的探索，我们才能正确地选择最优动作（optimal actions）。相反，如果没有探索某个动作，这个动作可能恰好是最优动作，从而被错过。  
在实践中，探索起始（exploring starts）是很难实现的。对于许多应用，尤其是涉及与环境物理交互的应用，很难收集到从每一对状态-行动开始的 episode。  
### ε-greedy policies
好处：平衡开发（（exploitation）与探索（exploration）  
策略改进（policy improvement）步骤改为求解，在求解上面这个问题的时候，不是在所有的策略里面去找，只是在 Πε 里面去找。其中，Πε 表示具有固定ε值的所有ε-greedy 策略的集合（这里 ε 是事先给定的）。这时候所得到的最优策略是：（把最大的概率仍然给 greedy action，但是会给其他所有 action 都给一个相同的比较小的概率）  
技巧：如果想用 MC ε-Greedy的话，那么 ε 选择不能太大。或者用一个技巧，在最开始的时候 ε 比较大，探索性比较强，最后 ε 逐渐减小到 0，就可以得到一个最优的策略




